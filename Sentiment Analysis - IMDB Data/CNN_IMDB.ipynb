{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN - IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHx5v06MWGmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "ece63c78-8901-4560-f684-4101db2486ff"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\r\u001b[K     |██████                          | 10kB 24.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 31.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 36.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 21.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 20.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.14.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp36-none-any.whl size=73200 sha256=6986d45007daec1e4ee478985c7d8a878075e9b940386b19aab7860e1b41ed5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=01c40cd35e3490fce0738278bfe4c9af993605fe76c16482b7fede6017a2b93d\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.3 keras-tuner-1.0.1 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSvDjgpUW3Mt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b65a8bb7-4fb7-46a3-9642-53066af5f5ab"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv',names=['sentence','label'])\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence     label\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISouI3wjW3Jl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.loc[df['label'] == 'positive' , 'label'] = 1\n",
        "df.loc[df['label'] == 'negative' , 'label'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvZBs5AFW3HP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df.sentence.values\n",
        "y = df.label.values\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjz5Jq4q6W0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0c7cc31-eef2-47e1-e945-807582168f3b"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q80vNYjRW3Cc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "656097cf-da6b-4849-e053-a4fa9d084905"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(X_train[2])\n",
        "print(X_train_seq[2])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "To experience Head you really need to understand where the Monkees were when they filmed it.<br /><br />This was as their series was coming to a close and the group was near break up. Their inventive and comedic series (sort of an American Idol of their day) took four unknown actors and formed a manufactured supergroup around them.<br /><br />This is their take on their \"manufactured image\" and status as the 2nd tier Beatles. They always felt they were in a box, trapped, and unable to find credibility despite their talents.<br /><br />It is also a hell of a musical-trippy, inventive (I have the soundtrack) and full of surprises.<br /><br />See it with an open mind.\n",
            "[5, 577, 391, 21, 65, 351, 5, 403, 117, 1, 7688, 68, 50, 33, 755, 9, 7, 7, 11, 13, 14, 62, 222, 13, 545, 5, 3, 502, 2, 1, 555, 13, 854, 1034, 53, 62, 4377, 2, 1736, 222, 436, 4, 32, 271, 6743, 4, 62, 246, 541, 680, 1537, 151, 2, 5305, 3, 11138, 31119, 183, 91, 7, 7, 11, 6, 62, 193, 20, 62, 11138, 1345, 2, 2739, 14, 1, 3498, 16689, 5306, 33, 209, 442, 33, 68, 8, 3, 879, 2323, 2, 1895, 5, 176, 3499, 454, 62, 2200, 7, 7, 9, 6, 85, 3, 587, 4, 3, 699, 11139, 4377, 10, 25, 1, 693, 2, 390, 4, 2382, 7, 7, 63, 9, 15, 32, 862, 331]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFUjNKCVWY-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cba2534-1827-4803-bd9e-071fbd7d296e"
      },
      "source": [
        "max_length = max([len(s.split()) for s in X])\n",
        "print(max_length)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1830\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHRSnYqsWZKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d5b74e9-358c-42b6-a63d-0bbdf19d4525"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=max_length)\n",
        "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=max_length)\n",
        "X_train_pad[0,:]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 10, 192, 129, ...,   0,   0,   0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6jMC9XMWZsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "    with open(filepath,encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix[idx] = np.array(\n",
        "                    vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgnqYnLSWZIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim3 = 100\n",
        "embedding_matrix = create_embedding_matrix('/content/glove.6B.100d.txt', tokenizer.word_index, embedding_dim3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XspwkYThWZGK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "2b1c12db-d928-4ccb-823b-4f0fc97f1234"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "from keras.layers import Dense,Conv1D,Flatten,MaxPooling1D\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.layers import GlobalMaxPool1D\n",
        "\n",
        "embedding_dim3 = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim3, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=max_length, \n",
        "                           trainable=True))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1830, 100)         5448700   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1826, 128)         64128     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 5,514,129\n",
            "Trainable params: 5,514,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzP5vOL8WZCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "epochs = 5\n",
        "embedding_dim = 100\n",
        "maxlen = max_length\n",
        "output_file = 'output.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IMQj5CN1Be0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_filters, kernel_size, vocab_size, embedding_dim, maxlen):\n",
        "    model = Sequential()\n",
        "    model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen,weights=[embedding_matrix]))\n",
        "    model.add(layers.Conv1D(num_filters, kernel_size, activation='relu'))\n",
        "    model.add(layers.GlobalMaxPooling1D())\n",
        "    model.add(layers.Dense(10, activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIPbrPKO1Bbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb5c9d77-bf92-48ef-f4a7-9f913b0c7396"
      },
      "source": [
        "param_grid = dict(num_filters=[32, 64, 128],\n",
        "                      kernel_size=[3, 5, 7],\n",
        "                      vocab_size=[vocab_size],\n",
        "                      embedding_dim=[embedding_dim],\n",
        "                      maxlen=[maxlen])\n",
        "model = KerasClassifier(build_fn=create_model,\n",
        "                            epochs=epochs, batch_size=32)\n",
        "grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n",
        "                              cv=4, verbose=1, n_iter=5)\n",
        "grid_result = grid.fit(X_train_pad, y_train)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 3s 524us/step - loss: 0.6720 - accuracy: 0.6262\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 3s 496us/step - loss: 0.4149 - accuracy: 0.8462\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 3s 488us/step - loss: 0.2353 - accuracy: 0.9213\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 3s 490us/step - loss: 0.1024 - accuracy: 0.9798\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 3s 494us/step - loss: 0.0325 - accuracy: 0.9982\n",
            "2000/2000 [==============================] - 0s 105us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 3s 508us/step - loss: 0.6568 - accuracy: 0.6305\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 3s 495us/step - loss: 0.3951 - accuracy: 0.8375\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 3s 496us/step - loss: 0.2308 - accuracy: 0.9117\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 3s 493us/step - loss: 0.1101 - accuracy: 0.9685\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 3s 497us/step - loss: 0.0344 - accuracy: 0.9945\n",
            "2000/2000 [==============================] - 0s 98us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 3s 515us/step - loss: 0.6195 - accuracy: 0.6572\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 3s 497us/step - loss: 0.3535 - accuracy: 0.8607\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 3s 502us/step - loss: 0.1761 - accuracy: 0.9487\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 3s 495us/step - loss: 0.0647 - accuracy: 0.9902\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 3s 492us/step - loss: 0.0208 - accuracy: 0.9990\n",
            "2000/2000 [==============================] - 0s 97us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 3s 509us/step - loss: 0.6397 - accuracy: 0.6475\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 3s 487us/step - loss: 0.3895 - accuracy: 0.8483\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 3s 492us/step - loss: 0.2138 - accuracy: 0.9295\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 3s 487us/step - loss: 0.0961 - accuracy: 0.9803\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 3s 489us/step - loss: 0.0308 - accuracy: 0.9978\n",
            "2000/2000 [==============================] - 0s 98us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 5s 778us/step - loss: 0.6153 - accuracy: 0.6698\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 4s 738us/step - loss: 0.3241 - accuracy: 0.8610\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 4s 737us/step - loss: 0.1579 - accuracy: 0.9480\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 4s 737us/step - loss: 0.0574 - accuracy: 0.9917\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 4s 736us/step - loss: 0.0161 - accuracy: 0.9995\n",
            "2000/2000 [==============================] - 0s 166us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 5s 767us/step - loss: 0.6475 - accuracy: 0.6435\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 4s 741us/step - loss: 0.4763 - accuracy: 0.8318\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 4s 750us/step - loss: 0.2362 - accuracy: 0.9185\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 4s 748us/step - loss: 0.0931 - accuracy: 0.9795\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 4s 743us/step - loss: 0.0264 - accuracy: 0.9977\n",
            "2000/2000 [==============================] - 0s 166us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 5s 763us/step - loss: 0.5614 - accuracy: 0.7145\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 4s 738us/step - loss: 0.2854 - accuracy: 0.8900\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 4s 741us/step - loss: 0.1317 - accuracy: 0.9635\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 4s 743us/step - loss: 0.0446 - accuracy: 0.9957\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 4s 749us/step - loss: 0.0138 - accuracy: 0.9995\n",
            "2000/2000 [==============================] - 0s 175us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 5s 766us/step - loss: 0.6222 - accuracy: 0.6520\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 4s 745us/step - loss: 0.3394 - accuracy: 0.8615\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 4s 747us/step - loss: 0.1730 - accuracy: 0.9452\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 4s 746us/step - loss: 0.0637 - accuracy: 0.9898\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 4s 742us/step - loss: 0.0190 - accuracy: 0.9992\n",
            "2000/2000 [==============================] - 0s 163us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 5s 850us/step - loss: 0.6933 - accuracy: 0.5397\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 5s 831us/step - loss: 0.5922 - accuracy: 0.7170\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 5s 817us/step - loss: 0.4243 - accuracy: 0.8825\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 5s 819us/step - loss: 0.3049 - accuracy: 0.9545\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 5s 827us/step - loss: 0.2340 - accuracy: 0.9803\n",
            "2000/2000 [==============================] - 0s 185us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 5s 835us/step - loss: 0.5557 - accuracy: 0.7018\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 5s 827us/step - loss: 0.2579 - accuracy: 0.9107\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 5s 824us/step - loss: 0.1073 - accuracy: 0.9767\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 5s 829us/step - loss: 0.0307 - accuracy: 0.9987\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 5s 826us/step - loss: 0.0103 - accuracy: 1.0000\n",
            "2000/2000 [==============================] - 0s 185us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 5s 841us/step - loss: 0.6896 - accuracy: 0.5400\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 5s 825us/step - loss: 0.5630 - accuracy: 0.7605\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 5s 822us/step - loss: 0.3120 - accuracy: 0.9010\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 5s 825us/step - loss: 0.1055 - accuracy: 0.9720\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 5s 826us/step - loss: 0.0305 - accuracy: 0.9963\n",
            "2000/2000 [==============================] - 0s 187us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 5s 836us/step - loss: 0.5788 - accuracy: 0.6825\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 5s 817us/step - loss: 0.2862 - accuracy: 0.8957\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 5s 828us/step - loss: 0.1267 - accuracy: 0.9707\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 5s 817us/step - loss: 0.0370 - accuracy: 0.9973\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 5s 814us/step - loss: 0.0117 - accuracy: 0.9998\n",
            "2000/2000 [==============================] - 0s 188us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 4s 695us/step - loss: 0.5512 - accuracy: 0.7157\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 4s 673us/step - loss: 0.3084 - accuracy: 0.8743\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 4s 670us/step - loss: 0.1611 - accuracy: 0.9498\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 4s 672us/step - loss: 0.0579 - accuracy: 0.9913\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 4s 663us/step - loss: 0.0172 - accuracy: 0.9995\n",
            "2000/2000 [==============================] - 0s 145us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 4s 691us/step - loss: 0.5861 - accuracy: 0.6925\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 4s 665us/step - loss: 0.3220 - accuracy: 0.8678\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 4s 662us/step - loss: 0.1843 - accuracy: 0.9383\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 4s 664us/step - loss: 0.0819 - accuracy: 0.9828\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 4s 678us/step - loss: 0.0284 - accuracy: 0.9982\n",
            "2000/2000 [==============================] - 0s 147us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 4s 702us/step - loss: 0.6092 - accuracy: 0.6853\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 4s 675us/step - loss: 0.3324 - accuracy: 0.8758\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 4s 676us/step - loss: 0.1869 - accuracy: 0.9393\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 4s 675us/step - loss: 0.0855 - accuracy: 0.9833\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 4s 690us/step - loss: 0.0307 - accuracy: 0.9983\n",
            "2000/2000 [==============================] - 0s 151us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 4s 699us/step - loss: 0.6668 - accuracy: 0.6053\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 4s 686us/step - loss: 0.3952 - accuracy: 0.8412\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 4s 669us/step - loss: 0.2157 - accuracy: 0.9233\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 4s 675us/step - loss: 0.0981 - accuracy: 0.9762\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 4s 675us/step - loss: 0.0317 - accuracy: 0.9967\n",
            "2000/2000 [==============================] - 0s 148us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 3s 549us/step - loss: 0.6820 - accuracy: 0.5933\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 3s 527us/step - loss: 0.4401 - accuracy: 0.8167\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 3s 536us/step - loss: 0.1957 - accuracy: 0.9380\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 3s 528us/step - loss: 0.0627 - accuracy: 0.9890\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 3s 529us/step - loss: 0.0168 - accuracy: 0.9997\n",
            "2000/2000 [==============================] - 0s 110us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 3s 552us/step - loss: 0.6415 - accuracy: 0.6325\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 3s 530us/step - loss: 0.3751 - accuracy: 0.8552\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 3s 529us/step - loss: 0.1938 - accuracy: 0.9393\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 3s 534us/step - loss: 0.0737 - accuracy: 0.9872\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 3s 529us/step - loss: 0.0233 - accuracy: 0.9995\n",
            "2000/2000 [==============================] - 0s 118us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 3s 546us/step - loss: 0.6143 - accuracy: 0.6567\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 3s 517us/step - loss: 0.3477 - accuracy: 0.8627\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 3s 519us/step - loss: 0.1719 - accuracy: 0.9470\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 3s 522us/step - loss: 0.0655 - accuracy: 0.9898\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 3s 518us/step - loss: 0.0195 - accuracy: 0.9995\n",
            "2000/2000 [==============================] - 0s 112us/step\n",
            "Epoch 1/5\n",
            "6000/6000 [==============================] - 3s 546us/step - loss: 0.6670 - accuracy: 0.6097\n",
            "Epoch 2/5\n",
            "6000/6000 [==============================] - 3s 526us/step - loss: 0.4384 - accuracy: 0.8190\n",
            "Epoch 3/5\n",
            "6000/6000 [==============================] - 3s 528us/step - loss: 0.1975 - accuracy: 0.9338\n",
            "Epoch 4/5\n",
            "6000/6000 [==============================] - 3s 531us/step - loss: 0.0722 - accuracy: 0.9860\n",
            "Epoch 5/5\n",
            "6000/6000 [==============================] - 3s 525us/step - loss: 0.0219 - accuracy: 0.9987\n",
            "2000/2000 [==============================] - 0s 114us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  6.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "8000/8000 [==============================] - 6s 766us/step - loss: 0.5032 - accuracy: 0.7454\n",
            "Epoch 2/5\n",
            "8000/8000 [==============================] - 6s 742us/step - loss: 0.2460 - accuracy: 0.9029\n",
            "Epoch 3/5\n",
            "8000/8000 [==============================] - 6s 752us/step - loss: 0.1020 - accuracy: 0.9724\n",
            "Epoch 4/5\n",
            "8000/8000 [==============================] - 6s 743us/step - loss: 0.0268 - accuracy: 0.9977\n",
            "Epoch 5/5\n",
            "8000/8000 [==============================] - 6s 746us/step - loss: 0.0073 - accuracy: 0.9999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DID4dAB1BTu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bcb2af24-de28-4842-8bd7-dd0db8a5a4a9"
      },
      "source": [
        "print(grid_result.best_score_)\n",
        "print(grid_result.best_params_)\n",
        "test_accuracy = grid.score(X_test_pad, y_test)\n",
        "print(test_accuracy)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8743750005960464\n",
            "{'vocab_size': 54487, 'num_filters': 128, 'maxlen': 1830, 'kernel_size': 5, 'embedding_dim': 100}\n",
            "2000/2000 [==============================] - 0s 180us/step\n",
            "0.8730000257492065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0TP1b7m5YUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "6fa0f813-0602-4a09-b42a-f830d02d2cec"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
        "                           weights=[embedding_matrix], \n",
        "                           input_length=max_length, \n",
        "                           ))\n",
        "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_44 (Embedding)     (None, 1830, 100)         5448700   \n",
            "_________________________________________________________________\n",
            "conv1d_44 (Conv1D)           (None, 1826, 128)         64128     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_44 (Glo (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_87 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 5,514,129\n",
            "Trainable params: 5,514,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRTfd0615YwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "72332e27-7f73-4c1c-9a7f-5751216e3aa6"
      },
      "source": [
        "model.fit(X_train_pad,y_train,epochs=5,batch_size=64,validation_split=0.2)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6400 samples, validate on 1600 samples\n",
            "Epoch 1/5\n",
            "6400/6400 [==============================] - 4s 664us/step - loss: 4.3749e-04 - accuracy: 1.0000 - val_loss: 6.3243e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/5\n",
            "6400/6400 [==============================] - 4s 661us/step - loss: 3.9479e-04 - accuracy: 1.0000 - val_loss: 6.0918e-04 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "6400/6400 [==============================] - 4s 662us/step - loss: 3.5792e-04 - accuracy: 1.0000 - val_loss: 5.9273e-04 - val_accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "6400/6400 [==============================] - 4s 656us/step - loss: 3.2523e-04 - accuracy: 1.0000 - val_loss: 5.8948e-04 - val_accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "6400/6400 [==============================] - 4s 659us/step - loss: 2.9767e-04 - accuracy: 1.0000 - val_loss: 5.5899e-04 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f0d916159e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G1FzR9_6Qq0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD-gWbj66Q_z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "09793d14-ac8f-492f-c0df-288ee2dfac13"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "y_pred = model.predict(X_test_pad)\n",
        "y_pred = y_pred.round()\n",
        "y_test = y_test.astype('float32')\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(accuracy_score(y_test,y_pred))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[838 141]\n",
            " [102 919]]\n",
            "0.8785\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}